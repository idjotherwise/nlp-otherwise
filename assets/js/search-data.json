{
  
    
        "post0": {
            "title": "Making a fastapi app with sqlmodel",
            "content": "We will be deploying a quick fastapi app to Heroku, using sqlmodel to handle a connection to the database and alembic for migrations. Let&#39;s get straight into it! . To see the full code, see the git repo. . Package requirements . We will start by installing all the packages we will need into a virtual environment. . python3 -m venv fastenv source fastenv/bin/activate pip install fastapi sqlmodel uvicorn Jinja2 aiofiles . Those packages are enough to get off the ground for local development. We will be using an sqlite database for local development, but when we deploy to Heroku we&#39;ll use Postgres so let&#39;s install some additional things we need for later. . pip install psycopg2 alembic gunicorn . We also installed alembic for database migrations, and gunicorn to use as a production server. The psycopg2 package allows sqlmodel (which uses SQLAlchemy underneath) to talk to Postgres databases. . Those are all the packages we are going to need, so let&#39;s save them to a requirements.txt file before we forget. . pip freeze &gt; requirements.txt . Fastapi app . Start by making the files main.py and __init__.py inside a directory called app, so that the file structure looks like this: . . ├── app │   ├── __init__.py │   ├── main.py ├── bookenv │   ├── bin │   ├── include │   ├── lib │   └── pyvenv.cfg └── requirements.txt . Main.py . from fastapi import FastAPI import uvicorn app = FastAPI() @app.get(&quot;/&quot;) def home_page(): return {&quot;message&quot;: &quot;Hello!&quot;} if __name__==&quot;__main__&quot;: uvicorn.run(&#39;app.main:app&#39;, reload=True) . Running this with python -m app.main should start up a server on port 8000 on your localhost, so going to 127.0.0.1:8000 should show you the message . {&quot;message&quot;: &quot;Hello!&quot;} . Making it look nice . Before setting up our database and models, let&#39;s make the first page look nice using Jinja templates. . from fastapi import FastAPI import uvicorn ## new from starlette.staticfiles import StaticFiles from starlette.templating import Jinja2Templates from starlette.requests import Request templates = Jinja2Templates(&#39;app/templates&#39;) ###### app = FastAPI() ## new def configure(): app.mount(&#39;/static&#39;, StaticFiles(directory=&#39;app/static&#39;), name=&#39;static&#39;) ###### ## changed @app.get(&quot;/&quot;) def home_page(request: Request): return templates.TemplateResponse(&#39;index.html&#39;, {&#39;request&#39;: request}) ### if __name__==&quot;__main__&quot;: configure() ## new uvicorn.run(&#39;app.main:app&#39;, reload=True) else: configure() ## new . After importing the required packages, we set up the templates with Jinja2Templates(&#39;app/templates&#39;) and also mount the static files so that fastapi can find them with app.mount(&#39;/static&#39;, StaticFiles(directory=&#39;app/static&#39;, name=&#39;static&#39;), so we need to make 2 directories: . mkdir app/static mkdir app/templates . Inside templates, we&#39;ll create the following index.html file, . &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en-us&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta name=&quot;description&quot; content=&quot;A website showcasing fastapi and sqlmodel&quot;&gt; &lt;meta name=&quot;author&quot; content=&quot;idjotherwise&quot;&gt; &lt;title&gt;Fastsqlmodel showcase&lt;/title&gt; &lt;link href=&quot;//oss.maxcdn.com/libs/twitter-bootstrap/3.0.3/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt; A super stylish home page &lt;/h1&gt; &lt;p&gt; Really interesting content, full of buzzwords like AI and MACHINE LEARNING! &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; . Now we can run the app to check that things are working so far, . python -m main.app . If everything has gone well, after opening up localhost:8000, you should be greeted with . . We&#39;ll refactor the jinja templates later, but that will do for now. . Next we&#39;ll set up some database stuff, including some models to hold our data. . Database: sqlmodel . In a new file called database.py, we&#39;ll set up the database engine and session handler. . # app/database.py from sqlmodel import SQLModel, create_engine, Session from .settings import settings connect_args = {&quot;check_same_thread&quot;: False} engine = create_engine(settings.database_url), connect_args=connect_args) def create_db_and_tables(): SQLModel.metadata.create_all(engine) def get_session(): with Session(engine) as session: yield session . and in the file settings.py we&#39;ll define the DATABASE_URL, . # app/settings.py from pydantic import BaseSettings class Settings(BaseSettings): database_url: str = &#39;sqlite:///database.db&#39; . This may seem uncessarily complex, but it will make things easier later. When your class Settings inherits from BaseSettings, it will first try to get the attributes from your environment but if it doesn&#39;t find anything it will set it to the default you&#39;ve given. So in our case (assuming you haven&#39;t set DATABASE_URL to anything else in your terminal) it will simply be the sqlite database url. On Heroku, environment variables like DATABASE_URL are set automatically (if you have installed a Postgres add-on). . Before we actually create the database, we should create a model that will go into the database. In a new file models.py, . # app/models.py from sqlmodel import SQLModel, Field from typing import Optional class BookBase(SQLModel): title: str author: Optional[str] class Book(BookBase, table=True): id: Optional[int] = Field(default=None, primary_key=True) . In sqlmodel, models can be treated as both Pydantic models and SQLAlchemy models. That&#39;s all it takes to create the table called Book in the database. Now let&#39;s actually create the table, we add some code to main.py . # app/main.py # changed from fastapi import FastAPI, Depends # - import uvicorn from starlette.staticfiles import StaticFiles from starlette.templating import Jinja2Templates from starlette.requests import Request # new from sqlmodel import Session, select from .database import create_db_and_tables, get_session from .models import Book # templates = Jinja2Templates(&#39;app/templates&#39;) app = FastAPI() def configure(): # new create_db_and_tables() # - app.mount(&#39;/static&#39;, StaticFiles(directory=&#39;app/static&#39;), name=&#39;static&#39;) # changed @app.get(&quot;/&quot;) def home_page(*, session: Session = Depends(get_session), request: Request): books = session.exec(select(Book)).all() return templates.TemplateResponse(&#39;index.html&#39;, {&#39;request&#39;: request}) # - if __name__==&quot;__main__&quot;: configure() uvicorn.run(&#39;app.main:app&#39;, reload=True) else: configure() . Finally add the following bit of code to the index.html file (see the Jinja documentation for more information), . &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en-us&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta name=&quot;description&quot; content=&quot;A website showcasing fastapi and sqlmodel&quot;&gt; &lt;meta name=&quot;author&quot; content=&quot;idjotherwise&quot;&gt; &lt;title&gt;Fastsqlmodel showcase&lt;/title&gt; &lt;link href=&quot;//oss.maxcdn.com/libs/twitter-bootstrap/3.0.3/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;/head&gt; &lt;body style=&quot;text-align: center&quot;&gt; &lt;h1&gt; A super stylish home page &lt;/h1&gt; &lt;p&gt; Really interesting content, full of buzzwords like AI and MACHINE LEARNING! &lt;/p&gt; &lt;/body&gt; &lt;/html&gt; . At this point, if you navigate localhost:8000, you won&#39;t see anything has changed - that&#39;s because our database is empty. But if the database did have books in it, then in the get request the books = session.exec(select(Book)).all() would contain a list of book items, and so our Jinja template index.html would get passed a non-empty list . &lt;h1&gt; A super stylish home page &lt;/h1&gt; &lt;p&gt; Really interesting content, full of buzzwords like AI and MACHINE LEARNING! &lt;/p&gt; . There are many ways to add data into our database. You could use SQLite Browser to open the database.db file or any other database browser. Instead, we will add a POST route so that we can add in data from the browser. . Add the following code the main.py, just below the @app.get(&#39;/&#39;) route. . # new @app.post(&quot;/book&quot;) def add_book(*, session: Session = Depends(get_session), request: Request, book: BookCreate) -&gt; BookRead: db_book = Book.from_orm(book) session.add(db_book) session.commit() session.refresh(db_book) return db_book # - . We&#39;ve added in two new models here, BookCreate and BookRead. Add them to the models.py file. . # app/models.py from sqlmodel import SQLModel, Field from typing import Optional class BookBase(SQLModel): title: str author: Optional[str] class Book(BookBase, table=True): id: Optional[int] = Field(default=None, primary_key=True) # new class BookCreate(BookBase): pass class BookRead(BookBase): id: int # . Now, navigate to localhost:8000/docs in your browser, and you should see the POST method right there: . . Click on the post request and add a few books in. These will now be saved into the database. Thanks to the way we wrote the index.html file, if you navigate again to the home page (localost:8000), you should see the books that you just added. Since we didn&#39;t implement @app.delete or @app.patch methods, we can&#39;t delete or update the entries. To start afresh, you can just delete the database.db file and re-add the books. . Adding a description column with Alembic . Now let&#39;s say that we want to add a description field to the book items. We could just delete the database.db file, make the changes to the Book model, and restart the app. However, if we were in the situation where the database had a bunch of entries that you don&#39;t want to lose, then using migrations is the way to go. We&#39;ll be using alembic, see the documentation for more information. We need to do a few changes to the setup since we are using sqlmodel and not sqlalchemy. TestDriven.io has a nice tutorial showing how to do this, so we&#39;ll follow that here (although we won&#39;t be using the async version) Start by going to the terminal, from root directory of the project, run . alembic init alembic . This will make a directory called alembic with some files that we need to modify. The project structure should now look like this . . |-- alembic | |-- README | |-- env.py | |-- script.py.mako | `-- versions |-- alembic.ini |-- app | |-- __init__.py | |-- database.py | |-- main.py | |-- models.py | |-- settings.py | `-- static/ | `-- templates | `-- index.html |-- database.db `-- requirements.txt . In the script.py.mako file, we need to import sqlmodel: . # alembic/script.py.mako # code above ommited from alembic import op import sqlalchemy as sa # new import sqlmodel # - ${imports if imports else &quot;&quot;} # code below ommited . and finally in the alembic/env.py add the following imports, . from logging.config import fileConfig from sqlalchemy import engine_from_config from sqlalchemy import pool from alembic import context # new from sqlmodel import SQLModel from app.settings import settings from app.models import Book # - # this is the Alembic Config object, which provides # access to the values within the .ini file in use. config = context.config # new config.set_main_option(&quot;sqlalchemy.url&quot;, settings.database_url) # - # Interpret the config file for Python logging. # This line sets up loggers basically. fileConfig(config.config_file_name) # add your model&#39;s MetaData object here # for &#39;autogenerate&#39; support # target_metadata = mymodel.Base.metadata target_metadata = SQLModel.metadata # - code below omitted . All we&#39;ve done here is import the SQLModel from sqlmodel, which holds all the metadata about our database (after also importing our Book model), then we&#39;ve grabbed the database_url from the settings file. . Now delete the database.db file, and then generate the first alembic migration with alembic revision -autogenerate -m &quot;Add book table&quot; to verify that everything sets up properly. If successful, you will find a new file in the alembic/revisions directory which looks something like this (the revision ID might be different): . &quot;&quot;&quot;Add book table Revision ID: 7071875a0907 Revises: Create Date: 2021-09-12 22:16:26.595677 &quot;&quot;&quot; from alembic import op import sqlalchemy as sa # new import sqlmodel # - # revision identifiers, used by Alembic. revision = &#39;7071875a0907&#39; down_revision = None branch_labels = None depends_on = None def upgrade(): # ### commands auto generated by Alembic - please adjust! ### op.create_table(&#39;book&#39;, sa.Column(&#39;title&#39;, sqlmodel.sql.sqltypes.AutoString(), nullable=False), sa.Column(&#39;author&#39;, sqlmodel.sql.sqltypes.AutoString(), nullable=True), sa.Column(&#39;id&#39;, sa.Integer(), nullable=True), sa.PrimaryKeyConstraint(&#39;id&#39;) ) op.create_index(op.f(&#39;ix_book_author&#39;), &#39;book&#39;, [&#39;author&#39;], unique=False) op.create_index(op.f(&#39;ix_book_id&#39;), &#39;book&#39;, [&#39;id&#39;], unique=False) op.create_index(op.f(&#39;ix_book_title&#39;), &#39;book&#39;, [&#39;title&#39;], unique=False) # ### end Alembic commands ### def downgrade(): # ### commands auto generated by Alembic - please adjust! ### op.drop_index(op.f(&#39;ix_book_title&#39;), table_name=&#39;book&#39;) op.drop_index(op.f(&#39;ix_book_id&#39;), table_name=&#39;book&#39;) op.drop_index(op.f(&#39;ix_book_author&#39;), table_name=&#39;book&#39;) op.drop_table(&#39;book&#39;) # ### end Alembic commands ### . Apply the migration (which just creates the table in the database) with alembic upgrade head. . Now to add a new column to the book table in the database, add the following to the Book model in app/models.py, . ... class BookBase(SQLModel): title: str author: Optional[str] # new description: Optional[str] # - ... . and then generate another migration file with . alembic revision --autogenerate -m &quot;Add description column&quot; . This gives us the following migration file, . &quot;&quot;&quot;Add description column Revision ID: 0b26b04f3af4 Revises: 7071875a0907 Create Date: 2021-09-12 22:55:29.481379 &quot;&quot;&quot; from alembic import op import sqlalchemy as sa import sqlmodel # revision identifiers, used by Alembic. revision = &#39;0b26b04f3af4&#39; down_revision = &#39;7071875a0907&#39; branch_labels = None depends_on = None def upgrade(): # ### commands auto generated by Alembic - please adjust! ### op.add_column(&#39;book&#39;, sa.Column(&#39;description&#39;, sqlmodel.sql.sqltypes.AutoString(), nullable=True)) op.create_index(op.f(&#39;ix_book_description&#39;), &#39;book&#39;, [&#39;description&#39;], unique=False) # ### end Alembic commands ### def downgrade(): # ### commands auto generated by Alembic - please adjust! ### op.drop_index(op.f(&#39;ix_book_description&#39;), table_name=&#39;book&#39;) op.drop_column(&#39;book&#39;, &#39;description&#39;) # ### end Alembic commands ### . Apply the migration with alembic upgrade head. . Update Jinja template . Now we are going to modify the jinja template in index.html to use the new description column. While we&#39;re at it, lets add some more styling to the page with a header and a footer. For this, we&#39;ll create a new file templates/layout.html. In there we will put all the things from index.html that should go on every page (if there were more pages). . &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en-us&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta name=&quot;description&quot; content=&quot;A website showcasing fastapi and sqlmodel&quot;&gt; &lt;meta name=&quot;author&quot; content=&quot;idjotherwise&quot;&gt; &lt;title&gt;Fastsqlmodel showcase&lt;/title&gt; &lt;link href=&quot;//oss.maxcdn.com/libs/twitter-bootstrap/3.0.3/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;link href=&quot;/static/css/styling.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;/head&gt; &lt;body &gt; &lt;div class=&quot;content-wrapper&quot;&gt; &lt;div class=&quot;container&quot;&gt; &lt;div class=&quot;row&quot;&gt; &lt;h1&gt; &lt;span class=&quot;font-semi-bold&quot;&gt; Book list &lt;/span&gt; &lt;/h1&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;row&quot;&gt; &lt;div class=&quot;col-md-10&quot;&gt; &lt;div&gt; {% block content %} &lt;div class=&quot;content&quot;&gt; THIS PAGE HAS NO CONTENT &lt;/div&gt; {% endblock %} &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&quot;footer&quot;&gt; &lt;div class=&quot;footer links&quot;&gt; &lt;ul&gt; &lt;li&gt;&lt;i class=&quot;glyphicon glyphicon-cog icon-muted&quot;&gt;&lt;/i&gt;&lt;a href=&quot;https://github.com/idjotherwise&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;i class=&quot;glyphicon glyphicon-globe icon-muted&quot;&gt;&lt;/i&gt;&lt;a href=&quot;https://twitter.com&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; . Now our app/templates/index.html will extend the app/templates/layout.html file: . {% extends &quot;layout.html&quot; %} {% block content %} &lt;h1&gt; A super stylish home page &lt;/h1&gt; &lt;p&gt; Really interesting content, full of buzzwords like AI and MACHINE LEARNING! &lt;/p&gt; {% if books %} &lt;ul style=&quot;text-align: left&quot;&gt; {% for book in books %} &lt;li key=&quot;{{book.id}}&quot;&gt;Title: {{book.title}}, Author: {{book.author}}&lt;/li&gt; &lt;!-- New description --&gt; &lt;p&gt;{{book.description}}&lt;/p&gt; {% endfor %} &lt;/ul&gt; {% endif %} {% endblock %} . Finally we will add some simple styling in the file app/static/css/styling.css. You can also just put in your own style file here if you like. . @import url(//fonts.googleapis.com/css?family=Open+Sans:300,400,600,700); body { font-family: &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-weight: 300; color: black; background: white; padding-left: 20px; padding-right: 20px; } h1, h2, h3, h4, h5, h6 { font-family: &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif; font-weight: 300; } p { font-weight: 300; } .content-wrapper { margin-top: 25px; height: 100vh; text-align: center; } .content-wrapper .content h1 span { margin-top: 10px; font-size: 60px; text-align: left; } . That&#39;s enough front-end stuff for now, next up we&#39;ll work on adding some tests (with pytest) and then deploying the initial app to Heroku. . Testing with Pytest . Deploy to Heroku . Add a file called Procfile in the root of the directory, and add the following to it: . web: gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker release: alembic upgrade head . When we push our code to Heroku, it will detect the presence of this Procfile and run the web command as the main process and the release command in the release phase of the app. . Adding other endpoints .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/web-dev/fastapi/2021/09/12/fastapi-sqlmodel.html",
            "relUrl": "/web-dev/fastapi/2021/09/12/fastapi-sqlmodel.html",
            "date": " • Sep 12, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ECDC's Covid traffic-light system",
            "content": "The European Center for Disease Prevention and Control (ECDC) publishes situation updates on COVID-19 for each country in the EU. Part of this reporting relates to the traffic-light system which informs the travel restrictions between countries within the EU (see for example https://reopen.europa.eu/en, which has a tool to say what the restrictions are between any two countries). The rules for each traffic light color is the following (see the ecdc website, updated 14 June 2021): . Green: if the 14-day notification rate is less than 50 and the test positivity rate is less than 4%; or | if the 14-day notification rate is less than 75 and the test positivity rate less than 1% | . | Orange: if the 14-day notification rate is less than 50 and the test positivity rate is 4% or more; or | the 14-day notification rate is 50 or more and less than 75 and the test positivity rate is 1% or more; or | the 14-day notification rate is between 75 and 200 and the test positivity rate is less than 4% | . | Red: if the 14-day cumulative COVID-19 case notification rate ranges from 75 to 200 and the test positivity rate of tests for COVID-19 infection is 4% or more; or | if the 14-day cumulative COVID-19 case notification rate is more than 200 but less than 500 | . | Dark red: if the 14-day cumulative COVID-19 case notification rate is 500 or more | . | Grey: if there is insufficient information or if the testing rate is lower than 300 cases per 100 000. | . | . And here it is in picture form: . . filter_all_nations = [ &quot;areaType=nation&quot; ] filter_all_uk = [ &quot;areaType=overview&quot; ] structure_cases_death = { &quot;date&quot;: &quot;date&quot;, &quot;areaName&quot;: &quot;areaName&quot;, &quot;newCases&quot;: &quot;newCasesByPublishDate&quot;, &quot;cumCases&quot;: &quot;cumCasesBySpecimenDate&quot;, &quot;cumCasesRate&quot;: &quot;cumCasesBySpecimenDateRate&quot;, &quot;newDeaths&quot;: &quot;newDeathsByDeathDate&quot;, &quot;newTests&quot;: &quot;newTestsByPublishDate&quot; } uk_cases = Cov19API(filters=filter_all_nations, structure=structure_cases_death).get_dataframe().fillna(0) uk_cases[&#39;date&#39;] = pd.to_datetime(uk_cases[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) uk_cases.sort_values([&#39;areaName&#39;, &#39;date&#39;], inplace=True) uk_cases.reset_index(drop=True, inplace=True) date_list = [&#39;2020-12-13&#39;, &#39;2020-12-14&#39;, &#39;2020-12-15&#39;, &#39;2020-12-16&#39;, &#39;2020-12-17&#39;] uk_cases.iloc[(uk_cases.query(&quot;areaName==&#39;Wales&#39;&quot;).query(&quot;date==@date_list&quot;).index), 2] = np.flip( np.array(list(range(2494 + int((2801 - 2494)/6), 2801 - int((2801 - 2494)/6), int((2801 - 2494)/6))))) countries = [&#39;Wales&#39;, &#39;Scotland&#39;, &#39;Northern Ireland&#39;, &#39;England&#39;] countries_population = dict() for country in countries: countries_population[country] = round(100000 * uk_cases.query( &quot;areaName == @country&quot;).cumCases.max() / uk_cases.query(&quot;areaName == @country&quot;).cumCasesRate.max()) if &#39;population&#39; not in uk_cases.columns: countries_pop_df = pd.DataFrame.from_dict(countries_population, orient=&#39;index&#39;, columns=[ &#39;population&#39;]) uk_cases = uk_cases.join(countries_pop_df, on=&#39;areaName&#39;) uk_cases[&#39;newCasesRate&#39;] = 100000 * uk_cases.newCases / uk_cases.population uk_cases[&#39;weeklyCasesRate&#39;] = uk_cases.groupby(by=&#39;areaName&#39;)[&#39;newCasesRate&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) uk_cases[&#39;twoWeeklyCasesRate&#39;] = uk_cases.groupby(by=&#39;areaName&#39;)[&#39;newCasesRate&#39;].rolling(14).sum().reset_index(drop=True).fillna(0) uk_cases[&#39;weeklyTests&#39;] = uk_cases.groupby(by=&#39;areaName&#39;)[&#39;newTests&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) uk_cases[&#39;weeklyCases&#39;] = uk_cases.groupby(by=&#39;areaName&#39;)[&#39;newCases&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) uk_cases[&#39;testPositivity&#39;] = 100 * uk_cases[&#39;weeklyCases&#39;] / uk_cases[&#39;weeklyTests&#39;] overview_cases = Cov19API(filters=filter_all_uk, structure=structure_cases_death).get_dataframe().fillna(0) def preprocess_dataframe(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) df.sort_values(&#39;date&#39;, inplace=True) df.reset_index(drop=True, inplace=True) df[&#39;casesChange&#39;] = df[&#39;newCases&#39;] - df[&#39;newCases&#39;].shift(-1).fillna(0) population = round(100000 * df.cumCases.max() / df.cumCasesRate.max()) df[&#39;newCasesRate&#39;] = 100000 * df.newCases / population df[&#39;casesChangeRate&#39;] = 100000 * df.casesChange / population df[&#39;weeklyCasesRate&#39;] = df[&#39;newCasesRate&#39;].rolling(7).sum().fillna(0) df[&#39;twoWeeklyCasesRate&#39;] = df[&#39;newCasesRate&#39;].rolling(14).sum().fillna(0) df[&#39;weeklyTests&#39;] = df.groupby(by=&#39;areaName&#39;)[&#39;newTests&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) df[&#39;weeklyCases&#39;] = df.groupby(by=&#39;areaName&#39;)[&#39;newCases&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) df[&#39;testPositivity&#39;] = 100 * df[&#39;weeklyCases&#39;] / df[&#39;weeklyTests&#39;] return df preprocess_dataframe(overview_cases) pass . . Summary: . def traffic_light(x, y): if y &gt;= 500: return &#39;&lt;span style=&quot;color: darkred;&quot;&gt;Dark Red&lt;/span&gt;&#39; if x &lt; 1: if y &lt; 75: return &#39;&lt;span style=&quot;color: green;&quot;&gt;Green&lt;/span&gt;&#39; if y &lt; 200: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; if 1 &lt;= x &lt; 4: if y &lt; 50: return &#39;&lt;span style=&quot;color: green;&quot;&gt;Green&lt;/span&gt;&#39; if y &lt;= 200: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; if x &gt;= 4: if y &lt; 75: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; else: return &#39;&lt;span style=&quot;color: grey;&quot;&gt;Grey&lt;/span&gt;&#39; strings = [] for country in countries: country_data = uk_cases.query(&#39;areaName == @country&#39;) positivity, caserate = country_data.testPositivity.iloc[-1], country_data.twoWeeklyCasesRate.iloc[-1] previous_caserate = country_data.twoWeeklyCasesRate.iloc[-2] previous_text = f&quot;(&lt;span style=&#39;color: green&#39;&gt;⬇&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; if previous_caserate &gt; caserate else f&quot;(&lt;span style=&#39;color:red;&#39;&gt;⬆&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; text = traffic_light(positivity,caserate) strings.append(f&quot;{country} is {text} - Positivity: {positivity:.3}%, 14 day incidence: {caserate:.2f} {previous_text}&quot;) for s in strings: display(Markdown(s)) positivity, caserate = overview_cases.testPositivity.iloc[-1], overview_cases.twoWeeklyCasesRate.iloc[-1] text = traffic_light(positivity,caserate) previous_caserate = overview_cases.twoWeeklyCasesRate.iloc[-2] previous_text = f&quot;(&lt;span style=&#39;color: green&#39;&gt;⬇&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; if previous_caserate &gt; caserate else f&quot;(&lt;span style=&#39;color:red;&#39;&gt;⬆&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; display(Markdown(f&quot;Overall, the UK is {text} - Positivity: {positivity:.2}%, 14 day incidence: {caserate:.2f} {previous_text}&quot;)) . . Wales is Orange - Positivity: inf%, 14 day incidence: 19.09 (⬇ from 20.51 the day before) . Scotland is Red - Positivity: 66.0%, 14 day incidence: 115.75 (⬆ from 63.37 the day before) . Northern Ireland is Red - Positivity: 8.78%, 14 day incidence: 206.65 (⬇ from 209.49 the day before) . England is Red - Positivity: 12.5%, 14 day incidence: 107.08 (⬆ from 97.81 the day before) . Overall, the UK is Red - Positivity: 4.5%, 14 day incidence: 185.62 (⬇ from 192.68 the day before) . Plots . First a plot with each country in the UK. The top right shows the positivity rate for (which is the percentage of registered tests which were positive) while the top left is the 14 day incidence rate. The bottom chart can be used to select the timeframe (does not work on a mobile device). . brush = alt.selection(type=&#39;interval&#39;, name=&#39;DateBrush&#39;,encodings=[&#39;x&#39;],fields=[&#39;Time&#39;]) leg_selection = alt.selection_multi(fields=[&#39;areaName&#39;], bind=&#39;legend&#39;) base = alt.Chart(uk_cases).mark_line().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;twoWeeklyCasesRate:Q&quot;, axis=alt.Axis(title=&#39;14 day incidence rate&#39;)), tooltip=[&#39;areaName&#39;, &#39;date&#39;,&#39;twoWeeklyCasesRate&#39;, &#39;testPositivity&#39;], color=&#39;areaName&#39;, opacity=alt.condition(leg_selection, alt.value(2), alt.value(0.1)) ).add_selection(leg_selection).properties(width=350) upper = base.encode(alt.X(&#39;yearmonthdate(date):T&#39;, axis=alt.Axis(title=&#39;&#39;), scale=alt.Scale(domain=brush)) ).properties(title=f&#39;14 day incidence rate of UK&#39;) upper_right = upper.encode(alt.Y(&#39;testPositivity:Q&#39;, axis=alt.Axis(title=&#39;Test Positivity&#39;))).properties(title=&#39;Test positivity&#39;) lower = base.properties(height=60, width = 760).add_selection(brush) (upper | upper_right) &amp; lower . . def plot_traffic_light(country: str): brush = alt.selection(type=&#39;interval&#39;, name=&#39;DateBrush&#39;,encodings=[&#39;x&#39;],fields=[&#39;Time&#39;]) leg_selection = alt.selection_multi(fields=[&#39;testPositivity&#39;], bind=&#39;legend&#39;) base = alt.Chart(uk_cases.query(&quot;areaName==@country&quot;)).mark_point(size=2).encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;twoWeeklyCasesRate:Q&quot;, axis=alt.Axis(title=&#39;14 day incidence rate&#39;)), tooltip=[&#39;areaName&#39;, &#39;date&#39;,&#39;twoWeeklyCasesRate&#39;, &#39;testPositivity&#39;], color=alt.condition(alt.datum.testPositivity &lt;= 4,alt.value(&#39;green&#39;), alt.value(&#39;orange&#39;)) ) upper = base.encode(alt.X(&#39;yearmonthdate(date):T&#39;, axis=alt.Axis(title=&#39;&#39;),scale=alt.Scale(domain=brush) ), color= alt.condition(alt.datum.twoWeeklyCasesRate &lt;= 25, alt.value(&#39;green&#39;), alt.value(&#39;orange&#39;) ), ).properties(title=f&#39;14 day incidence rate&#39;).properties(width=300) upper_right = upper.encode(alt.Y(&#39;testPositivity:Q&#39;, axis=alt.Axis(title=&#39;Test Positivity&#39;)), color=alt.condition(alt.datum.testPositivity &lt;= 4,alt.value(&#39;green&#39;), alt.value(&#39;orange&#39;))).properties(title=&#39;Test positivity&#39;).properties(width=300) lower = alt.layer( base.encode(color=alt.condition((alt.datum.testPositivity &lt; 4) &amp; (alt.datum.twoWeeklyCasesRate &gt;= 50) &amp; (alt.datum.twoWeeklyCasesRate &lt; 200), alt.ColorValue(&#39;orange&#39;), alt.ColorValue(&#39;red&#39;) ), opacity=alt.condition((alt.datum.twoWeeklyCasesRate &gt;= 50) &amp; (alt.datum.twoWeeklyCasesRate &lt; 500), alt.value(1), alt.value(0) ) ), base.encode(color=alt.value(&#39;darkred&#39;), opacity=alt.condition(alt.datum.twoWeeklyCasesRate &gt;= 500, alt.value(1), alt.value(0) ) ), base.encode(color=alt.condition((alt.datum.testPositivity &lt; 4) &amp; (alt.datum.twoWeeklyCasesRate &lt; 50), alt.ColorValue(&#39;green&#39;), alt.ColorValue(&#39;orange&#39;) ), opacity=alt.condition(alt.datum.twoWeeklyCasesRate &lt; 50, alt.value(1), alt.value(0) ) ) ).add_selection(brush).properties(height=120, width=660) return (upper | upper_right) &amp; lower . . Traffic Lights . The next 4 charts show&#39;s the same data in abit more detail. The points are color coded to indicate when the &#39;traffic light&#39; would change color. In the top left and right charts, this is just done on the basis of &#39;green&#39; or &#39;not green&#39; - so &#39;green&#39; when the positivity rate is below $4 %$ and &#39;green&#39; when the 14 day incidence rate is below $25$. The bottom chart then combines all the cases and shows what color traffic light the countries would be according to the ECDC. Note that while the incidence rate goes back to the start of the pandemic, the testing only started being recorded at the beginning of July 2020, so the colouring before that date wouldn&#39;t necessarily make much sense. . Wales . plot_traffic_light(&#39;Wales&#39;) . England . plot_traffic_light(&#39;England&#39;) . Scotland . plot_traffic_light(&#39;Scotland&#39;) . Northern Ireland . plot_traffic_light(&#39;Northern Ireland&#39;) .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/covid/2021/05/06/ecdc-trafficlight.html",
            "relUrl": "/covid/2021/05/06/ecdc-trafficlight.html",
            "date": " • May 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "European Covid data exploration",
            "content": "Importing and preparing the data . We will be looking at data from the following countries: . Italy | Austria | Germany | Belgium | France | United Kingdom | Portugal | . We begin by importing the data, and adding some new features so that we can compare the data from different countries. For example we calculate &#39;confirmed cases per 100k population&#39;, &#39;deaths per 100k&#39; and &#39;new cases&#39; since these are not initially in the dataset. This data is collected and preprocessed in this file. . import altair as alt import pandas as pd x_small_url = &quot;https://raw.githubusercontent.com/idjotherwise/nlp-otherwise/master/data_sets/european_covid.csv&quot; . . Here is a random sample of 5 rows from the dataset. . x_small = pd.read_csv(x_small_url) x_small.sample(5) . id date vaccines confirmed tests recovered deaths population confirmed_per deaths_per ratio tests_per vaccines_per new_cases new_cases_per . 2583 Portugal | 2021-12-29 | 19177996.0 | 1330158.0 | 26078371.0 | 1175217.0 | 18921.0 | 10283822.0 | 12934.471250 | 183.988015 | 1.422463 | 2.535864e+05 | 1.864870 | 26867.0 | 261.255008 | . 4571 France | 2022-06-18 | 145413200.0 | 29236926.0 | 278238869.0 | NaN | 145754.0 | 66977107.0 | 43652.118328 | 217.617641 | 0.498527 | 4.154238e+05 | 2.171088 | 0.0 | 0.000000 | . 5285 Austria | 2021-11-04 | 11792419.0 | 861429.0 | 98968990.0 | 776593.0 | 14077.0 | 8840521.0 | 9744.097661 | 159.232697 | 1.634145 | 1.119493e+06 | 1.333905 | 9767.0 | 110.479914 | . 7008 Switzerland | 2021-04-06 | 1672961.0 | 610837.0 | 5868819.0 | NaN | 9994.0 | 8513227.0 | 7175.152266 | 117.393792 | 1.636116 | 6.893765e+04 | 0.196513 | 2690.0 | 31.597889 | . 2690 Portugal | 2022-04-15 | NaN | NaN | 40700808.0 | NaN | NaN | 10283822.0 | NaN | NaN | NaN | 3.957751e+05 | NaN | 0.0 | 0.000000 | . Plotting the data . We will first look at the total numbers of cases and deaths in each country, before moving on to cases and deaths per 100k population. . . In each of the charts below, you can click on the legend to filter the lines shown . Total cases per 100,000 . leg_selection = alt.selection_multi(fields=[&#39;id&#39;], bind=&#39;legend&#39;) alt.Chart(x_small_url).mark_line().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;confirmed_per:Q&quot;, axis=alt.Axis(title=&#39;Confirmed per 100k&#39;)), tooltip=[&#39;id:N&#39;, &#39;confirmed_per:Q&#39;], color=alt.Color(&#39;id:N&#39;, legend=alt.Legend(title=&quot;Countries&quot;)), opacity=alt.condition(leg_selection, alt.value(1), alt.value(0.2)) ).transform_filter(alt.datum.confirmed_per&gt;0).add_selection(leg_selection).properties(title=&#39;Total number of cases per 100,000 population for selected European Countries&#39;, width=600).interactive() . . Total deaths per 100,000 . alt.Chart(x_small_url).mark_line().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;deaths_per:Q&quot;, axis=alt.Axis(title=&#39;Deaths per 100k&#39;), impute=alt.ImputeParams(value=50)), tooltip=[&quot;id:N&quot;, &quot;deaths_per:Q&quot;, &quot;yearmonthdate(date):T&quot;], color=alt.Color(&#39;id:N&#39;, legend=alt.Legend(title=&quot;Countries&quot;)), opacity=alt.condition(leg_selection, alt.value(1), alt.value(0.2)) ).transform_filter(alt.datum.deaths_per&gt;0).add_selection(leg_selection).properties(title=&#39;Number of deaths per 100,000 population for selected European Countries&#39;, width=600).interactive() . . Two week incidence rate . brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(x_small_url).mark_line().transform_filter(alt.datum.new_cases_per&gt;0).transform_window( rolling_mean=&#39;sum(new_cases_per)&#39;, frame=[-14, 0], groupby=[&#39;id:N&#39;] ).encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;) ), y=alt.Y(&quot;rolling_mean:Q&quot;, axis=alt.Axis(title=&#39;Incidence rate&#39;) ), tooltip=[&#39;id:N&#39;, &#39;rolling_mean:Q&#39;], color=alt.Color(&#39;id:N&#39;, legend=alt.Legend(title=&quot;Countries&quot;)), opacity=alt.condition(leg_selection, alt.value(1), alt.value(0.2)) ).add_selection(leg_selection).properties( width=600, height=400, title=&#39;Number of new cases per 100,000 over two weeks for selected countries&#39; ) upper = base.encode( alt.X(&#39;yearmonthdate(date):T&#39;,axis=alt.Axis(title=&#39;Date&#39;), scale=alt.Scale(domain=brush)) ) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . The ratio of confirmed cases and deaths gives an indication of what the case fatality rate is - it seems to be between 2 and 3%, assuming that the countries listed here are catching all positive cases (which they probably aren&#39;t, so it&#39;s likely lower than this). . Case fatality rate . base = alt.Chart(x_small_url).mark_line().transform_filter(alt.datum.ratio&gt;0).encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;ratio:Q&quot;, axis=alt.Axis(title=&#39;Ratio of deaths per case&#39;)), tooltip=&#39;id:N&#39;, color=alt.Color(&#39;id:N&#39;, legend=alt.Legend(title=&quot;Countries&quot;)), opacity=alt.condition(leg_selection, alt.value(1), alt.value(0.2)) ).add_selection(leg_selection).properties(title=&#39;The ratio of deaths to confirmed cases (case fatality rate)&#39;, width=600) upper = base.encode( alt.X(&#39;yearmonthdate(date):T&#39;,axis=alt.Axis(title=&#39;Date&#39;), scale=alt.Scale(domain=brush)) ) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Vaccines . In the chart below we plot the number vaccines given per population - this means that if the number is 1, then the country has given the equivalent of 1 shot for each person in the country. Since not everyone in the countries are eligible to get the vaccine, a ratio of 1 means that many people have recieved two jabs. Note also that some kinds of vaccines (the J&amp;J&#39;s Janssen vaccine, for example) only require 1 shot so the goal is not neccesarily to reach exactly 2 shots per person in the whole country. . alt.Chart(x_small_url).mark_line().transform_filter(alt.datum.vaccines_per&gt;0).encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;vaccines_per:Q&quot;, axis=alt.Axis(title=&#39;Number of vaccines given&#39;)), tooltip=[&#39;id:N&#39;, &#39;vaccines_per:Q&#39;], color=alt.Color(&#39;id:N&#39;, legend=alt.Legend(title=&quot;Countries&quot;)), opacity=alt.condition(leg_selection, alt.value(1), alt.value(0.2)) ).add_selection(leg_selection).properties(title=&#39;Number of vaccines given&#39;, width=600).interactive() . .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/covid/2021/02/21/european-covid.html",
            "relUrl": "/covid/2021/02/21/european-covid.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Deaths in England due to Covid19",
            "content": "An Exception was encountered at &#39;In [4]&#39;. . In this post we will look at the number of cases and number of deaths due to Covid-19 in England, and we will use these numbers to estimate a few things: . The approximate number of cases that actually occured during the first wave (Winter and Spring of 2020) | The mortality rate (or rather, the number of people dying vs the number of positive cases) | The number death rate for the next few weeks based on the number of new cases over the last couple of weeks. | . Importing data . We will grab the data on the number of cases and deaths for each English region and also do some cleaning and feature engineering. . from uk_covid19 import Cov19API import pandas as pd import altair as alt import numpy as np #collapse filter_all_regions = [ &quot;areaType=region&quot; ] structure_deaths = { &quot;date&quot;: &quot;date&quot;, &quot;areaName&quot;: &quot;areaName&quot;, &quot;newCases&quot;: &quot;newCasesByPublishDate&quot;, &quot;newDeaths&quot;: &quot;newDeaths28DaysByPublishDate&quot; } eng_deaths = Cov19API(filters=filter_all_regions, structure=structure_deaths).get_dataframe().fillna(0) eng_deaths[&#39;date&#39;] = pd.to_datetime(eng_deaths[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) eng_deaths.sort_values([&#39;areaName&#39;, &#39;date&#39;], inplace=True) eng_deaths.reset_index(drop=True,inplace=True) eng_deaths[&#39;weeklyDeaths&#39;] = eng_deaths.groupby(by=&#39;areaName&#39;)[&#39;newDeaths&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) eng_deaths[&#39;weeklyCases&#39;] = eng_deaths.groupby(by=&#39;areaName&#39;)[&#39;newCases&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) eng_deaths[&#39;mortalityEstimated&#39;] = 100 *(eng_deaths.groupby(by=&#39;areaName&#39;)[&#39;weeklyDeaths&#39;].shift(-14))/eng_deaths[&#39;weeklyCases&#39;] . . Next we do the same for the whole of England. . filter_england = [ &quot;areaType=nation&quot;, &quot;areaName=England&quot; ] full_eng_deaths = Cov19API(filters=filter_england, structure=structure_deaths).get_dataframe().fillna(0) full_eng_deaths[&#39;date&#39;] = pd.to_datetime(full_eng_deaths[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) full_eng_deaths.sort_values([&#39;areaName&#39;, &#39;date&#39;], inplace=True) full_eng_deaths.reset_index(drop=True,inplace=True) full_eng_deaths[&#39;newDeaths&#39;].iloc[-1] = np.nan full_eng_deaths[&#39;newDeaths&#39;].iloc[-2] = np.nan full_eng_deaths[&#39;newDeaths&#39;].iloc[-3] = np.nan full_eng_deaths[&#39;laggedNewDeaths&#39;] = full_eng_deaths[&#39;newDeaths&#39;].shift(-7) full_eng_deaths[&#39;estimateCasesFromDeaths&#39;] = full_eng_deaths[&#39;laggedNewDeaths&#39;] * 50 full_eng_deaths[&#39;estimateDeathsFromCases&#39;] = full_eng_deaths[&#39;newCases&#39;] * 0.02 . . /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy self._setitem_single_block(indexer, value, name) . Plotting the data . We first start by looking at the number of deaths in each English region since the beginning of march, then estimate the mortality rate (rate of deaths per positive case) in each region. We then move on to look at the data for the whole of England. . Plotting: English regions . bars = alt.Chart(eng_deaths.query(&quot;date &gt;= &#39;2021-01-01&#39;&quot;)).mark_bar().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;weeklyDeaths:Q&quot;, axis=alt.Axis(title=&#39;Weekly number of deaths&#39;)), tooltip=&quot;newDeaths:Q&quot; ).properties(width=700) bars.facet(alt.Column(&#39;areaName&#39;, title=&#39;Region&#39;), columns=1).properties(title=&#39;Weekly number of deaths in each region&#39;) . . MaxRowsError Traceback (most recent call last) /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs) 361 copy = self.copy(deep=False) 362 original_data = getattr(copy, &#34;data&#34;, Undefined) --&gt; 363 copy.data = _prepare_data(original_data, context) 364 365 if original_data is not Undefined: /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/v4/api.py in _prepare_data(data, context) 82 # convert dataframes or objects with __geo_interface__ to dict 83 if isinstance(data, pd.DataFrame) or hasattr(data, &#34;__geo_interface__&#34;): &gt; 84 data = _pipe(data, data_transformers.get()) 85 86 # convert string input to a URLData /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows) 17 @curried.curry 18 def default_data_transformer(data, max_rows=5000): &gt; 19 return curried.pipe(data, limit_rows(max_rows=max_rows), to_values) 20 21 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/utils/data.py in limit_rows(data, max_rows) 82 &#34;than the maximum allowed ({}). &#34; 83 &#34;For information on how to plot larger datasets &#34; &gt; 84 &#34;in Altair, see the documentation&#34;.format(max_rows) 85 ) 86 return data MaxRowsError: The number of rows in your dataset is greater than the maximum allowed (5000). For information on how to plot larger datasets in Altair, see the documentation . alt.FacetChart(...) . Execution using papermill encountered an exception here and stopped: . bars = alt.Chart(eng_deaths.query(&quot;date &gt;= &#39;2021-01-01&#39;&quot;)).mark_bar().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;mortalityEstimated:Q&quot;, axis=alt.Axis(title=&#39;Implied estimated mortality&#39;)), tooltip=&quot;mortalityEstimated:Q&quot; ).properties(width=800) bars.facet(alt.Column(&#39;areaName&#39;, title=&#39;Region&#39;), columns=1).properties(title=&#39;Number of deaths as a percentage of number of cases&#39;) . . MaxRowsError Traceback (most recent call last) /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/v4/api.py in to_dict(self, *args, **kwargs) 361 copy = self.copy(deep=False) 362 original_data = getattr(copy, &#34;data&#34;, Undefined) --&gt; 363 copy.data = _prepare_data(original_data, context) 364 365 if original_data is not Undefined: /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/v4/api.py in _prepare_data(data, context) 82 # convert dataframes or objects with __geo_interface__ to dict 83 if isinstance(data, pd.DataFrame) or hasattr(data, &#34;__geo_interface__&#34;): &gt; 84 data = _pipe(data, data_transformers.get()) 85 86 # convert string input to a URLData /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/vegalite/data.py in default_data_transformer(data, max_rows) 17 @curried.curry 18 def default_data_transformer(data, max_rows=5000): &gt; 19 return curried.pipe(data, limit_rows(max_rows=max_rows), to_values) 20 21 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in pipe(data, *funcs) 625 &#34;&#34;&#34; 626 for func in funcs: --&gt; 627 data = func(data) 628 return data 629 /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/toolz/functoolz.py in __call__(self, *args, **kwargs) 301 def __call__(self, *args, **kwargs): 302 try: --&gt; 303 return self._partial(*args, **kwargs) 304 except TypeError as exc: 305 if self._should_curry(args, kwargs, exc): /opt/hostedtoolcache/Python/3.7.13/x64/lib/python3.7/site-packages/altair/utils/data.py in limit_rows(data, max_rows) 82 &#34;than the maximum allowed ({}). &#34; 83 &#34;For information on how to plot larger datasets &#34; &gt; 84 &#34;in Altair, see the documentation&#34;.format(max_rows) 85 ) 86 return data MaxRowsError: The number of rows in your dataset is greater than the maximum allowed (5000). For information on how to plot larger datasets in Altair, see the documentation . alt.FacetChart(...) . For each region in England, we plot the ration of new deaths (lagged by 7 days) over the number of new cases as a percentage. We only look at the dates in the second half of 2020 because the lack of testing capacity skewed the numbers too much (the mortality rate is certainly not above 10%...). Note that we still see an overestimate of the mortality (in particular in the North East and the South East) over the summer, likely due to low levels of testing capacity. In the end, the implied mortality rate seems to have settled down to around 2%. . Plotting: All of England . The first chart we look at has both the number of new cases and the number of new deaths in England. The blue line corresponds to the axis on the right hand side, the number of new deaths. The green bars are the number of new cases (axis on the left hand side). . base = alt.Chart(full_eng_deaths.query(&quot;date &gt;= &#39;2021-01-01&#39;&quot;)).encode(x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=None))).properties(title=&#39;Number of new cases and the number of new deaths in England&#39;,width=700) bars = base.mark_bar(color=&#39;#57A44C&#39;).encode( y=alt.Y(&quot;newCases:Q&quot;, axis=alt.Axis(title=&#39;Number of new cases&#39;, titleColor=&#39;#57A44C&#39;)), tooltip=&quot;newCases:Q&quot; ) line = base.mark_line(stroke=&#39;#5276A7&#39;).encode(y=alt.Y(&quot;newDeaths:Q&quot;, axis=alt.Axis(title=&#39;Number of new deaths&#39;, titleColor=&#39;#5276A7&#39;))) alt.layer(bars, line).resolve_scale(y=&#39;independent&#39;) . . base = alt.Chart(full_eng_deaths.query(&quot;date &gt;= &#39;2021-01-01&#39;&quot;)).encode(x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=None))).properties(width=700, title=&#39;Estimating the number of deaths from the number of cases vs actual number of deaths&#39;) bars = base.mark_bar(color=&#39;#57A44C&#39;).encode( y=alt.Y(&quot;estimateDeathsFromCases:Q&quot;, axis=alt.Axis(title=&#39;Estimated number of deaths&#39;, titleColor=&#39;#57A44C&#39;)), tooltip=&quot;newCases:Q&quot; ) line = base.mark_line(stroke=&#39;#5276A7&#39;).encode(y=alt.Y(&quot;laggedNewDeaths:Q&quot;, axis=alt.Axis(title=&#39;Actual number of new deaths&#39;, titleColor=&#39;#5276A7&#39;))) alt.layer(bars, line).resolve_scale(y=&#39;independent&#39;) . . In the above chart, the blue line (axis on the right hand side) indicates the number of deaths recorded lagged by 7 days. The green bars (with axis on the left hand side) are what the number of deaths would be if the mortality rate was 2% and if the number of positive cases exaclty reflects the true number of cases. Of course this is not the case (especially in the first half of 2020, before mass testing was available). However this rough estimate seems to line up pretty well from September onwards, so we will use this estimate. Note that we have lagged the time series for the number of deaths by 7 days, so that we can visualise the interaction between the number of new cases and number of new deaths. This also allows us to see a prediction for the number of deaths that might occur based on the number of new cases that have been seen in the last week. . base = alt.Chart(full_eng_deaths.query(&quot;date &gt;= &#39;2021-01-01&#39;&quot;)).encode(x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=None))).properties(width=700, title=&#39;Estimating the number of cases from the number of deaths, based on 2% mortality rate&#39;) bars = base.mark_bar(color=&#39;#57A44C&#39;).encode( y=alt.Y(&quot;newCases:Q&quot;, axis=alt.Axis(title=&#39;Reported number of new cases&#39;, titleColor=&#39;#57A44C&#39;)), tooltip=&quot;newCases:Q&quot; ) line = base.mark_line(stroke=&#39;#5276A7&#39;).encode(y=alt.Y(&quot;estimateCasesFromDeaths:Q&quot;, axis=alt.Axis(title=&#39;Estimated number of cases from the deaths&#39;, titleColor=&#39;#5276A7&#39;))) alt.layer(bars, line).resolve_scale(y=&#39;independent&#39;) . . In the above chart, we have reversed the prediction to try and visualise how many cases there might have been based on the number of deaths that we saw - again, using a mortality rate of 2%. . We can compare this to other attempts at predicting the number of cases during the first peak, for example the following chart from ourworldindata.org . . Note that these numbers are for the whole of the UK - if we further assume that the cases were distributed evenly accross the UK, then we can just adjust those numbers from ourworldindata by ~80%. With these adjustments in mind, our prediction of ~50,000 cases at the peak of the first wave seems to fit in pretty well. . Spanish Influenza of 1918 . Finally I saw worrying graph showing how the influenza pandemic of 1918 developed on the CDC&#39;s website. The axis aren&#39;t labeled so I&#39;m not sure what the actual numbers were during the 3 waves, but it is estimated the a third of the world&#39;s population was infected with the flu virus, which is approximately 600 million (the world population was about 1.8 Billion then) and it is estimated the around 50 million died. This would make the mortality rate to be around 8%. . .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/covid/eda/2021/01/22/covid-england.html",
            "relUrl": "/covid/eda/2021/01/22/covid-england.html",
            "date": " • Jan 22, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Austrian Covid data exploration",
            "content": "Importing the data . The data used in this post can be found at https://www.data.gv.at/covid-19/. After downloading the CSV file called CovidFaelle_Timeline.csv, we need to do some cleaning of the date column and split off some specific sets - the numbers for Austria, Volarlberg, Tirol and Wien. We also need to format the decimal numbers since the CSV file uses a &#39;,&#39; instead of a &#39;.&#39; as decimal. . data_url = &quot;https://raw.githubusercontent.com/idjotherwise/nlp-otherwise/master/data_sets/at_full_data.csv&quot; full_data = pd.read_csv(data_url, parse_dates=[&#39;Time&#39;]) current_date = full_data.iloc[-1,0] all_austria = full_data.query(&quot;BundeslandID==10&quot;).sort_values(by=&#39;Time&#39;) string_to_md = f&quot;- Note that the latest date in this data is {current_date}.&quot; display(Markdown(string_to_md)) latest_data = full_data.query(f&quot;Time&gt;=&#39;{str((current_date - datetime.timedelta(7)).date())}&#39;&quot;) saturdays_data = full_data.query(&quot;Time.dt.dayofweek == 5&quot;).copy() saturdays_data[&#39;rate_change&#39;]=saturdays_data.groupby(by=&#39;Bundesland&#39;).SiebenTageInzidenzFaelle.pct_change().replace(np.inf, np.nan).fillna(0)*100 . . Note that the latest date in this data is 2022-09-22 00:00:00. | . latest_rate_vbg = latest_data.query(&quot;Bundesland==&#39;Vorarlberg&#39;&quot;).SiebenTageInzidenzFaelle latest_rate_aus = latest_data.query(&quot;Bundesland==&#39;Österreich&#39;&quot;).SiebenTageInzidenzFaelle vbg_change = round(list(latest_rate_vbg)[-1] - list(latest_rate_vbg)[0]) / list(latest_rate_vbg)[0] aus_change = round(list(latest_rate_aus)[-1] - list(latest_rate_aus)[0]) / list(latest_rate_aus)[0] week_trend_vbg = f&#39;&lt;span style=&quot;color: green;&quot;&gt;Down&lt;/span&gt; {vbg_change:.1%}&#39; if vbg_change &lt; 0 else f&#39;&lt;span style=&quot;color: red;&quot;&gt;Up&lt;/span&gt; {vbg_change:.1%}&#39; week_trend_aus = f&#39;&lt;span style=&quot;color: green;&quot;&gt;Down&lt;/span&gt; {aus_change:.1%}&#39; if aus_change &lt; 0 else f&#39;&lt;span style=&quot;color: red;&quot;&gt;Up&lt;/span&gt;{aus_change:.1%}&#39; vbg_string = f&quot;Weekly trend in **Vorarlberg**: {week_trend_vbg} ({latest_rate_vbg.iloc[-1]:.4} cases per 100k). Percentage of ICU beds occupied: {full_data.query(&#39;BundeslandID==8&#39;).ICUTakenPercent.iloc[-1]/100:.1%}&quot; aus_string = f&quot;Weekly trend in **Austria**: {week_trend_aus} ({latest_rate_aus.iloc[-1]:.4} cases per 100k). Percentage of ICU beds occupied: {full_data.query(&#39;BundeslandID==10&#39;).ICUTakenPercent.iloc[-1]/100:.1%}&quot; display(Markdown(vbg_string)) display(Markdown(aus_string)) . . Weekly trend in Vorarlberg: Up 59.3% (364.8 cases per 100k). Percentage of ICU beds occupied: 0.0% . Weekly trend in Austria: Up35.6% (490.6 cases per 100k). Percentage of ICU beds occupied: 8.6% . def traffic_light(x, y): if y &gt;= 500: return &#39;&lt;span style=&quot;color: darkred;&quot;&gt;Dark Red&lt;/span&gt;&#39; if x &lt; 1: if y &lt; 75: return &#39;&lt;span style=&quot;color: green;&quot;&gt;Green&lt;/span&gt;&#39; if y &lt; 200: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; if 1 &lt;= x &lt; 4: if y &lt; 50: return &#39;&lt;span style=&quot;color: green;&quot;&gt;Green&lt;/span&gt;&#39; if y &lt;= 200: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; if x &gt;= 4: if y &lt; 75: return &#39;&lt;span style=&quot;color: orange;&quot;&gt;Orange&lt;/span&gt;&#39; if y &lt; 500: return &#39;&lt;span style=&quot;color: red;&quot;&gt;Red&lt;/span&gt;&#39; else: return &#39;&lt;span style=&quot;color: grey;&quot;&gt;Grey&lt;/span&gt;&#39; strings = [] for country in full_data.Bundesland.unique(): country_data = full_data.query(&#39;Bundesland == @country&#39;) positivity, caserate = country_data.Positivity.iloc[-1], country_data.TwoWeeklyCasesRate.iloc[-1] previous_caserate = country_data.TwoWeeklyCasesRate.iloc[-2] previous_text = f&quot;(&lt;span style=&#39;color: green&#39;&gt;⬇&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; if previous_caserate &gt; caserate else f&quot;(&lt;span style=&#39;color:red;&#39;&gt;⬆&lt;/span&gt; from {previous_caserate:.2f} the day before)&quot; text = traffic_light(positivity,caserate) strings.append(f&quot;{country} is {text} - Positivity: {positivity:.2}%, 14 day incidence: {caserate:.2f} {previous_text}&quot;) for s in strings: display(Markdown(s)) . . Burgenland is Dark Red - Positivity: 5.9%, 14 day incidence: 800.04 (⬆ from 780.32 the day before) . Kärnten is Dark Red - Positivity: 2.1e+01%, 14 day incidence: 850.81 (⬆ from 811.42 the day before) . Niederösterreich is Dark Red - Positivity: 2.2e+01%, 14 day incidence: 890.44 (⬆ from 861.05 the day before) . Oberösterreich is Dark Red - Positivity: 1.6e+01%, 14 day incidence: 990.20 (⬆ from 920.70 the day before) . Salzburg is Dark Red - Positivity: 2.5e+01%, 14 day incidence: 789.31 (⬆ from 743.76 the day before) . Steiermark is Dark Red - Positivity: 9.0%, 14 day incidence: 783.20 (⬆ from 745.41 the day before) . Tirol is Dark Red - Positivity: 2.8e+01%, 14 day incidence: 892.77 (⬆ from 839.41 the day before) . Vorarlberg is Dark Red - Positivity: 2.2e+01%, 14 day incidence: 593.99 (⬆ from 548.36 the day before) . Wien is Dark Red - Positivity: 3.1%, 14 day incidence: 821.84 (⬆ from 811.28 the day before) . Österreich is Dark Red - Positivity: 8.9%, 14 day incidence: 852.55 (⬆ from 815.26 the day before) . source = saturdays_data.query(&quot;Time &gt;= &#39;2021-01-31&#39;&quot;) alt.Chart(source).mark_circle( opacity=0.8, stroke=&#39;black&#39;, strokeWidth=1 ).encode( alt.X(&#39;yearmonthdate(Time):T&#39;, axis=alt.Axis(title=&#39;&#39;, labelAngle=-45)), alt.Y(&#39;Bundesland:N&#39;), alt.Size(&#39;rate_change:Q&#39;, scale=alt.Scale(range=[0, 500]), legend=alt.Legend(title=&#39;Percentage change&#39;) ), alt.Color(&#39;Bundesland:N&#39;, legend=None), tooltip=[&#39;Time&#39;, &#39;rate_change&#39;] ).properties( width=600, height=320, title=&#39;Percentage change of incidence rate from the previous week&#39; ) . . Plotting the data . Here is a historical plot of the &#39;traffic light colours&#39; in Austria, as defined by the European Centre for Disease Prevention and Control. See also my other post about the traffic light system for countries in the UK here, where I show what the rules are for the different traffic lights. . Notice how long Austria was in the &#39;Red&#39; (and &#39;Dark red&#39;) between September and May - almost 6 months! . base = alt.Chart(full_data.query(&quot;BundeslandID==10&quot;)).mark_point(size=2).encode( x=alt.X(&quot;yearmonthdate(Time):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;TwoWeeklyCasesRate:Q&quot;, axis=alt.Axis(title=&#39;Cases per 100k&#39;)), tooltip=[&#39;Time:T&#39;,&#39;TwoWeeklyCasesRate:Q&#39;, &#39;Positivity:Q&#39;] ).properties( title=&#39;Number of cases per 100,000 in Austria&#39;, width=800 ) chart_to_show = alt.layer( base.encode(color=alt.condition((alt.datum.Positivity &lt; 4) &amp; (alt.datum.TwoWeeklyCasesRate &gt;= 75) &amp; (alt.datum.TwoWeeklyCasesRate &lt; 200), alt.ColorValue(&#39;orange&#39;), alt.ColorValue(&#39;red&#39;) ), opacity=alt.condition((alt.datum.TwoWeeklyCasesRate &gt;= 75) &amp; (alt.datum.TwoWeeklyCasesRate &lt; 500), alt.value(1), alt.value(0) ) ), base.encode(color=alt.value(&#39;darkred&#39;), opacity=alt.condition(alt.datum.TwoWeeklyCasesRate &gt;= 500, alt.value(1), alt.value(0) ) ), base.encode(color=alt.condition((alt.datum.Positivity &lt; 4) &amp; (alt.datum.TwoWeeklyCasesRate &lt; 50), alt.ColorValue(&#39;green&#39;), alt.ColorValue(&#39;orange&#39;) ), opacity=alt.condition((alt.datum.TwoWeeklyCasesRate &lt; 75) &amp; (alt.datum.Positivity &gt;= 1), alt.value(1), alt.value(0) ) ), base.encode(color=alt.ColorValue(&#39;green&#39;), opacity=alt.condition((alt.datum.TwoWeeklyCasesRate &lt; 75) &amp; (alt.datum.Positivity &lt; 1), alt.value(1), alt.value(0) ) ) ) chart_to_show.interactive() . . Next we have the 7 day incidence rate for states of Vorarlberg, Tirol and Wien compared to all of Austria. If you are on a computer, you can drag select the date range of the next graph by selecting it on the small graph at the bottom. The bar chart in the middle shows the maximum incidence rate for each state within that date range, while the bar chart on the right shows the maximum incidence rate up to before the end of the date range selected. Selecting the state from the legend will highlight the corresponding line in the charts. . start_date = full_data[&#39;Time&#39;].min() end_date = full_data[&#39;Time&#39;].max() x_init = pd.to_datetime([start_date, end_date]).astype(int) / 1E6 leg_selection = alt.selection_multi(fields=[&#39;Bundesland&#39;], bind=&#39;legend&#39;) brush = alt.selection(type=&#39;interval&#39;, name=&#39;DateBrush&#39;,encodings=[&#39;x&#39;],fields=[&#39;Time&#39;], init={&#39;Time&#39;: list(x_init)}) base = alt.Chart(data_url).mark_line().encode( x=alt.X(&quot;yearmonthdate(Time):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;SiebenTageInzidenzFaelle:Q&quot;, axis=alt.Axis(title=&#39;Incidence rate&#39;)), tooltip=[&#39;Bundesland:N&#39;, &quot;SiebenTageInzidenzFaelle:Q&quot;, &#39;yearmonthdate(Time):T&#39;], color=&#39;Bundesland:N&#39;, opacity=alt.condition(leg_selection, alt.value(2), alt.value(0.1)) ).add_selection(leg_selection).properties(width=600) upper = base.encode( alt.X(&#39;yearmonthdate(Time):T&#39;,axis=alt.Axis(title=&#39;&#39;), scale=alt.Scale(domain=brush)) ).properties(title=&#39;7 day incidence rate for states in Austria&#39;) lower = base.properties( height=60, width=600 ).add_selection(brush) # bars_max = base.transform_filter(&#39;datum.Time &gt;= DateBrush.Time[0] &amp;&amp; datum.Time &lt;= DateBrush.Time[1]&#39;).mark_bar().encode( # y = alt.Y(&#39;SiebenTageInzidenzFaelle:Q&#39;, title=None), # x = alt.X(&#39;Bundesland:N&#39;, title=None), # color=&#39;Bundesland&#39;, # opacity=alt.condition(leg_selection, alt.value(2), alt.value(0)), # ).properties(width=150, title=&#39;Max incidence rate (selected)&#39;) # bars_current = base.transform_filter(&#39;datum.Time &lt;= DateBrush.Time[1]&#39;).mark_bar().encode( # y = alt.Y(&#39;SiebenTageInzidenzFaelle:Q&#39;, title=None), # x = alt.X(&#39;Bundesland:N&#39;, title=None), # color=&#39;Bundesland&#39;, # opacity=alt.condition(leg_selection, alt.value(2), alt.value(0)), # ).properties(width=150, title=&#39;Max incidence rate (all time)&#39;) # alt.vconcat(alt.hconcat(upper, bars_max, bars_current).resolve_scale(y=&#39;shared&#39;),lower) upper &amp; lower . . brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) states = full_data[&#39;Bundesland&#39;].unique() states.sort() selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Bundesland&#39;], init={&#39;Bundesland&#39;: &#39;Vorarlberg&#39;}, bind={&#39;Bundesland&#39;: alt.binding_select(options=states)} ) bars = alt.Chart(data_url).mark_bar().add_selection( selection ).encode( x=alt.X(&quot;yearmonthdate(Time):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;TwoWeeklyCasesRate:Q&quot;, axis=alt.Axis(title=&#39;Incidence rate&#39;)), tooltip=[&#39;TwoWeeklyCasesRate:Q&#39;, &#39;Bundesland:N&#39;, &#39;yearmonthdate(Time):T&#39;], opacity=alt.condition(selection, alt.value(1), alt.value(0)) ).properties(title=f&#39;14 day incidence rate of individual states vs rolling mean across Austria&#39;, width=800) line = alt.Chart(full_data.query(&quot;BundeslandID==10&quot;)).mark_line( color=&#39;red&#39;, size=2, ).transform_window( rolling_mean=&#39;mean(TwoWeeklyCasesRate)&#39;, frame=[-3, 3] ).encode( x=&#39;yearmonthdate(Time):T&#39;, y=&#39;rolling_mean:Q&#39;, tooltip=[&#39;TwoWeeklyCasesRate:Q&#39;, &#39;Bundesland&#39;, &#39;yearmonthdate(Time):T&#39;] ) base = alt.layer(bars, line) upper_bars = bars.encode( alt.X(&#39;yearmonthdate(Time):T&#39;,axis=alt.Axis(title=&#39;Date&#39;), scale=alt.Scale(domain=brush)) ) upper_line = line.encode( alt.X(&#39;yearmonthdate(Time):T&#39;,axis=alt.Axis(title=&#39;Date&#39;), scale=alt.Scale(domain=brush)) ) upper = alt.layer(upper_bars, upper_line) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Choose from the above dropdown menu to view the different states. . Test Positivity in each state . leg_selection = alt.selection_multi(fields=[&#39;Bundesland&#39;], bind=&#39;legend&#39;) brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(data_url).mark_line().encode( x=alt.X(&quot;yearmonthdate(Time):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;Positivity:Q&quot;, axis=alt.Axis(title=&#39;Tests per person&#39;)), tooltip=[&#39;Positivity:Q&#39;, &#39;Bundesland:N&#39;, &#39;yearmonthdate(Time):T&#39;], color=&#39;Bundesland:N&#39;, opacity=alt.condition(leg_selection, alt.value(2), alt.value(0.1)) ).add_selection(leg_selection).properties(width=700) upper = base.encode( alt.X(&#39;yearmonthdate(Time):T&#39;,axis=alt.Axis(title=&#39;&#39;), scale=alt.Scale(domain=brush)) ).properties(title=&#39;Weekly Test Positivity&#39;) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . . Utilization of ICU beds . leg_selection = alt.selection_multi(fields=[&#39;Bundesland&#39;], bind=&#39;legend&#39;) brush = alt.selection(type=&#39;interval&#39;, encodings=[&#39;x&#39;]) base = alt.Chart(data_url).mark_line().transform_window( rolling_mean=&#39;mean(ICUTakenPercent)&#39;, frame=[-7,0], groupby=[&#39;Bundesland:N&#39;] ).encode( x=alt.X(&quot;yearmonthdate(Time):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;rolling_mean:Q&quot;, axis=alt.Axis(title=&#39;ICU Occupacy (%)&#39;)), tooltip=[&#39;Bundesland:N&#39;, &#39;rolling_mean:Q&#39;, &#39;ICUTakenPercent:Q&#39;], color=&#39;Bundesland:N&#39;, opacity=alt.condition(leg_selection, alt.value(2), alt.value(0.1)) ).add_selection(leg_selection).properties(width=600) upper = base.encode( alt.X(&#39;yearmonthdate(Time):T&#39;,axis=alt.Axis(title=&#39;&#39;), scale=alt.Scale(domain=brush)) ).properties(title=&#39;Percentage of ICU beds taken (7 day mean)&#39;) lower = base.properties( height=60 ).add_selection(brush) upper &amp; lower . .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/covid/2020/12/12/austria-covid.html",
            "relUrl": "/covid/2020/12/12/austria-covid.html",
            "date": " • Dec 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Sentiment analysis fastai tutorial",
            "content": "Text tutorial from fastai . The material can be found on the fastai website . Packages and data . We begin by importing all the required packages from the fastai text module. . from fastai.text.all import * . In fastai, each module has an all script which allows us to import everything that is necessesary from that module, in the knowledge that it is safe to do so. . We will be using the IMDB dataset to fine-tune a sentiment analysis model, so lets download it now. . path = untar_data(URLs.IMDB) path.ls() . (#8) [Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/test&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/tmp_clas&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/imdb.vocab&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/unsup&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/models&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/README&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/tmp_lm&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/train&#39;)] . We see train and test folders, so let&#39;s check what is inside both of those. . (path/&#39;train&#39;).ls(),(path/&#39;test&#39;).ls() . ((#4) [Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/train/neg&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/train/pos&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/train/unsupBow.feat&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/train/labeledBow.feat&#39;)], (#3) [Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/test/neg&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/test/pos&#39;),Path(&#39;/Users/ifanjohnston/.fastai/data/imdb/test/labeledBow.feat&#39;)]) . Both have subfolders containing positive and negative comments (and also some Bow related files, which I guess is something to do with a bag of words model). This is a standard structure for datasets, and fastai has a built in method to deal with importing the files using the folder names as labels. . So we create a DataLoaders (which is just a collection of DataLoader objects) . dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . Let&#39;s check out a batch of these reviews . dls.show_batch() . text category . 0 xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero | pos | . 1 xxbos xxmaj polish film maker xxmaj walerian xxmaj borowczyk &#39;s xxmaj la xxmaj bête ( french , 1975 , aka xxmaj the xxmaj beast ) is among the most controversial and brave films ever made and a very excellent one too . xxmaj this film tells everything that &#39;s generally been hidden and denied about our nature and our sexual nature in particular with the symbolism and silence of its images . xxmaj the images may look wild , perverse , &quot; sick &quot; or exciting , but they are all in relation with the lastly mentioned . xxmaj sex , desire and death are very strong and primary things and dominate all the flesh that has a human soul inside it . xxmaj they interest and xxunk us so powerfully ( and by our nature ) that they are considered scary , unacceptable and something too wild to be | pos | . 2 xxbos xxmaj berlin - born in 1942 xxmaj margarethe von xxmaj trotta was an actress and now she is a very important director and writer . xxmaj she has been described , perhaps even unfairly caricatured , as a director whose commitment to bringing a woman &#39;s sensibility to the screen outweighs her artistic strengths . &quot; rosenstrasse , &quot; which has garnered mixed and even strange reviews ( the xxmaj new xxmaj york xxmaj times article was one of the most negatively aggressive reviews xxmaj i &#39;ve ever read in that paper ) is not a perfect film . xxmaj it is a fine movie and a testament to a rare xxunk of successful opposition to the genocidal xxmaj nazi regime by , of all peoples , generically powerless xxmaj germans demonstrating in a xxmaj berlin street . n n xxmaj co - writer von xxmaj trotta uses the actual | pos | . 3 xxbos xxmaj here are the matches . . . ( adv . = advantage ) n n xxmaj the xxmaj warriors ( ultimate xxmaj warrior , xxmaj texas xxmaj tornado and xxmaj legion of xxmaj doom ) v xxmaj the xxmaj perfect xxmaj team ( mr xxmaj perfect , xxmaj ax , xxmaj smash and xxmaj crush of xxmaj demolition ) : xxmaj ax is the first to go in seconds when xxmaj warrior splashes him for the pin ( 4 - 3 adv . xxmaj warriors ) . i knew xxmaj ax was n&#39;t a healthy man but if he was that unhealthy why bother have him on the card ? xxmaj this would be his last xxup ppv . xxmaj eventually , both xxmaj legion of xxmaj doom and xxmaj demolition job out cheaply via double disqualification ( 2 - 1 adv . xxmaj warriors ) . xxmaj perfect | neg | . 4 xxbos xxmaj in xxup nyc , seaman xxmaj michael o&#39;hara ( orson xxmaj welles ) rescues xxmaj elsa xxmaj bannister ( rita xxmaj hayworth ) from a mugging &amp; rape as she takes a horse &amp; carriage through xxmaj central xxmaj park -and lives to regret it . xxmaj xxunk - haired xxmaj hayworth &#39;s a platinum blonde in this one ; as dazzling as fresh - fallen snow -but nowhere near as pure … n n xxmaj to reveal any more of the convoluted plot in this seminal &quot; noir &quot; would be criminal . xxmaj it &#39;s as deceptive as the mirrors used to cataclysmic effect in the final scenes -but the film holds far darker secrets : xxmaj from the xxup ny xxmaj times : &quot; childhood xxmaj shadows : xxmaj the xxmaj hidden xxmaj story xxmaj of xxmaj the xxmaj black xxmaj dahlia xxmaj murder &quot; by | pos | . 5 xxbos xxmaj i &#39;ve rented and watched this movie for the 1st time on xxup dvd without reading any reviews about it . xxmaj so , after 15 minutes of watching xxmaj i &#39;ve noticed that something is wrong with this movie ; it &#39;s xxup terrible ! i mean , in the trailers it looked scary and serious ! n n i think that xxmaj eli xxmaj roth ( mr . xxmaj director ) thought that if all the characters in this film were stupid , the movie would be funny … ( so stupid , it &#39;s funny … ? xxup wrong ! ) xxmaj he should watch and learn from better horror - comedies such xxunk xxmaj night &quot; , &quot; the xxmaj lost xxmaj boys &quot; and &quot; the xxmaj return xxmaj of the xxmaj living xxmaj dead &quot; ! xxmaj those are funny ! n n &quot; | neg | . 6 xxbos xxup myra xxup breckinridge is one of those rare films that established its place in film history immediately . xxmaj praise for the film was absolutely nonexistent , even from the people involved in making it . xxmaj this film was loathed from day one . xxmaj while every now and then one will come across some maverick who will praise the film on philosophical grounds ( aggressive feminism or the courage to tackle the issue of xxunk ) , the film has not developed a cult following like some notorious flops do . xxmaj it &#39;s not hailed as a misunderstood masterpiece like xxup scarface , or trotted out to be ridiculed as a camp classic like xxup showgirls . n n xxmaj undoubtedly the reason is that the film , though outrageously awful , is not lovable , or even likable . xxup myra xxup breckinridge is just | neg | . 7 xxbos i felt duty bound to watch the 1983 xxmaj timothy xxmaj dalton / xxmaj zelah xxmaj clarke adaptation of &quot; jane xxmaj eyre , &quot; because xxmaj i &#39;d just written an article about the 2006 xxup bbc &quot; jane xxmaj eyre &quot; for xxunk . n n xxmaj so , i approached watching this the way xxmaj i &#39;d approach doing homework . n n i was irritated at first . xxmaj the lighting in this version is bad . xxmaj everyone / everything is washed out in a bright white xxunk light that , in some scenes , casts shadows on the wall behind the characters . n n xxmaj and the sound is poorly recorded . i felt like i was listening to a high school play . n n xxmaj and the pancake make - up is way too heavy . n n xxmaj and the sets do n&#39;t fully | pos | . 8 xxbos xxmaj pier xxmaj paolo xxmaj pasolini , or xxmaj pee - pee - pee as i prefer to call him ( due to his love of showing male genitals ) , is perhaps xxup the most overrated xxmaj european xxmaj marxist director - and they are thick on the ground . xxmaj how anyone can see &quot; art &quot; in this messy , cheap sex - romp concoction is beyond me . xxmaj some of the &quot; stories &quot; here could have come straight out of a soft - core porn film , and i am not even so much referring to the nudity but the simplistic and banal , often pointless stories . xxmaj anyone who enjoyed this relatively watchable but dumb oddity should really sink his teeth into the &quot; der xxmaj xxunk &quot; soft - porn xxmaj german 70s movie series , because that &#39;s what | neg | . Note that a fair amount of preprocessing has been done already on the reviews, and some extra tokens have been inserted: . xxbos indicates that it is the beginning of a review | xxmaj indicates that the following word should be capitalized | . Next we can define a learner which is suitable for text classification: . learner = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy) . Note that we are using the AWD_LSTM architecture, which stands for Adjusted stochastic gradient descent Weight Decay, Long-Short-Term Memory. The AWD part basically just means that the way that weights are adjusted is modified, while the LSTM part means that it can deal with both long and short dependencies (more notes on LSTM architecture coming soon). . The drop_mult parameters just controls the magnitude of the dropouts in the model. For the metrics that we will be tracking, we just take accuracy. . Now we can fine-tune our model for a couple of epochs: . learner.fine_tune(2, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.608062 | 0.428918 | 0.809920 | 2:52:42 | . . 0.00% [0/2 00:00&lt;00:00] epoch train_loss valid_loss accuracy time . . 90.26% [352/390 4:32:27&lt;29:24 0.3171] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learner.show_results() . learner.save(&#39;2epochs&#39;) . &lt;/div&gt; .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/sentiment/fastai/2020/11/29/sentiment.html",
            "relUrl": "/sentiment/fastai/2020/11/29/sentiment.html",
            "date": " • Nov 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Covid19 exploration",
            "content": ". Note: If you just would like to see the graphs, just use the link in the table of contents to go to the last sections! . UK Covid19 API . In this post we will explore the data found in the UK governments Covid API (the Python version), which can be found on their website. Lets import the Python module first and checkout the main function. . from uk_covid19 import Cov19API import pandas as pd import altair as alt import numpy as np . Collapse the following output to see the help documents for Cov19API. It tells us that it is a class with the parameters filters (a list of strings), structure (a dictionary with a str key and dict or str value) and latest_by (a str or None). The class also has a method called get_dataframe() which will return the data as a Pandas DataFrame. . help(Cov19API) . Help on class Cov19API in module uk_covid19.api_interface: class Cov19API(builtins.object) | Cov19API(filters: Iterable[str], structure: Dict[str, Union[dict, str]], latest_by: Union[str, NoneType] = None) | | Interface to access the API service for COVID-19 data in the United Kingdom. | | Parameters | - | filters: Iterable[str] | API filters. See the API documentations for additional | information. | | structure: Dict[str, Union[dict, str]] | Structure parameter. See the API documentations for | additional information. | | latest_by: Union[str, None] | Retrieves the latest value for a specific metric. [Default: ``None``] | | Methods defined here: | | __init__(self, filters: Iterable[str], structure: Dict[str, Union[dict, str]], latest_by: Union[str, NoneType] = None) | Initialize self. See help(type(self)) for accurate signature. | | __repr__ = __str__(self) | | __str__(self) | Return str(self). | | get_csv(self, save_as=None) -&gt; str | Provides full data (all pages) in CSV. | | .. warning:: | | Please make sure that the ``structure`` is not hierarchical as | CSV outputs are defined as 2D tables and as such, do not support | hierarchies. | | Parameters | - | save_as: Union[str, None] | If defined, the results will (also) be saved as a | file. [Default: ``None``] | | The value must be a path to a file with the correct | extension -- i.e. ``.csv`` for CSV). | | Returns | - | str | | Raises | | ValueError | If the structure is nested. | | Examples | -- | &gt;&gt;&gt; filters = [&#34;areaType=region&#34;] | &gt;&gt;&gt; structure = { | ... &#34;name&#34;: &#34;areaName&#34;, | ... &#34;newCases&#34;: &#34;newCasesBySpecimenDate&#34; | ... } | &gt;&gt;&gt; data = Cov19API( | ... filters=filters, | ... structure=structure, | ... latest_by=&#39;newCasesBySpecimenDate&#39; | ... ) | &gt;&gt;&gt; result = data.get_csv() | &gt;&gt;&gt; print(result) | name,newCases | East Midlands,0 | ... | | get_dataframe(self) | Provides the data as as ``pandas.DataFrame`` object. | | .. versionadded:: 1.2.0 | | .. warning:: | | The ``pandas`` library is not included in the dependencies of this | library and must be installed separately. | | Returns | - | DataFrame | | Raises | | ImportError | If the ``pandas`` library is not installed. | | get_json(self, save_as: Union[str, NoneType] = None, as_string: bool = False) -&gt; Union[dict, str] | Provides full data (all pages) in JSON. | | Parameters | - | save_as: Union[str, None] | If defined, the results will (also) be saved as a | file. [Default: ``None``] | | The value must be a path to a file with the correct | extension -- i.e. ``.json`` for JSON). | | as_string: bool | .. versionadded:: 1.1.4 | | If ``False`` (default), returns the data as a dictionary. | Otherwise, returns the data as a JSON string. | | Returns | - | Union[Dict, str] | | Examples | -- | &gt;&gt;&gt; filters = [&#34;areaType=region&#34;] | &gt;&gt;&gt; structure = { | ... &#34;name&#34;: &#34;areaName&#34;, | ... &#34;newCases&#34;: &#34;newCasesBySpecimenDate&#34; | ... } | &gt;&gt;&gt; data = Cov19API( | ... filters=filters, | ... structure=structure, | ... latest_by=&#39;newCasesBySpecimenDate&#39; | ... ) | &gt;&gt;&gt; result = data.get_json() | &gt;&gt;&gt; print(result) | {&#39;data&#39;: [{&#39;name&#39;: &#39;East Midlands&#39;, &#39;newCases&#39;: 0}, ... } | | get_xml(self, save_as=None, as_string=False) -&gt; xml.etree.ElementTree.Element | Provides full data (all pages) in XML. | | Parameters | - | save_as: Union[str, None] | If defined, the results will (also) be saved as a | file. [Default: ``None``] | | The value must be a path to a file with the correct | extension -- i.e. ``.xml`` for XML). | | as_string: bool | .. versionadded:: 1.1.4 | | If ``False`` (default), returns an ``ElementTree`` | object. Otherwise, returns the data as an XML string. | | Returns | - | xml.etree.ElementTree.Element | | Examples | -- | &gt;&gt;&gt; from xml.etree.ElementTree import tostring | &gt;&gt;&gt; filters = [&#34;areaType=region&#34;] | &gt;&gt;&gt; structure = { | ... &#34;name&#34;: &#34;areaName&#34;, | ... &#34;newCases&#34;: &#34;newCasesBySpecimenDate&#34; | ... } | &gt;&gt;&gt; data = Cov19API( | ... filters=filters, | ... structure=structure, | ... latest_by=&#39;newCasesBySpecimenDate&#39; | ... ) | &gt;&gt;&gt; result_xml = data.get_xml() | &gt;&gt;&gt; result_str = tostring(result_xml, encoding=&#39;unicode&#39;, method=&#39;xml&#39;) | &gt;&gt;&gt; print(result_str) | &lt;document&gt; | &lt;data&gt; | &lt;name&gt;East Midlands&lt;/name&gt; | &lt;newCases&gt;0&lt;/newCases&gt; | &lt;/data&gt; | ... | &lt;/document&gt; | | head(self) | Request header for the given input arguments (``filters``, | ``structure``, and ``lastest_by``). | | Returns | - | Dict[str, str] | | Examples | -- | &gt;&gt;&gt; filters = [&#34;areaType=region&#34;] | &gt;&gt;&gt; structure = { | ... &#34;name&#34;: &#34;areaName&#34;, | ... &#34;newCases&#34;: &#34;newCasesBySpecimenDate&#34; | ... } | &gt;&gt;&gt; data = Cov19API( | ... filters=filters, | ... structure=structure, | ... latest_by=&#39;newCasesBySpecimenDate&#39; | ... ) | &gt;&gt;&gt; head = data.head() | &gt;&gt;&gt; print(head) | {&#39;Cache-Control&#39;: &#39;public, max-age=60&#39;, &#39;Content-Length&#39;: &#39;0&#39;, | ... | } | | - | Static methods defined here: | | get_release_timestamp() -&gt; str | :staticmethod: | Produces the website timestamp in GMT. | | .. versionadded:: 1.2.0 | | This property supplies the website timestamp - i.e. the time at which the data | were released to the API and by extension the website. Please note that there | will be a difference between this timestamp and the timestamp produced using | the ``last_update`` property. The latter signifies the time at which the data | were deployed to the database, not the time at which they were released. | | .. note:: | | The output is extracted from the header and is accurate to | the miliseconds. | | .. warning:: | | The ISO-8601 standard requires a ``&#34;Z&#34;`` character to be added | to the end of the timestamp. This is a timezone feature and is | not recognised by Python&#39;s ``datetime`` library. It is, however, | most other libraries; e.g. ``pandas``. If you wish to parse the | timestamp using the the ``datetime`` library, make sure that you | remove the trailing ``&#34;Z&#34;`` character. | | Returns | - | str | Timestamp, formatted as ISO-8601. | | Examples | -- | &gt;&gt;&gt; release_timestamp = Cov19API.get_release_timestamp() | &gt;&gt;&gt; print(release_timestamp) | 2020-08-08T15:00:09.977840Z | | &gt;&gt;&gt; from datetime import datetime | &gt;&gt;&gt; release_timestamp = Cov19API.get_release_timestamp() | &gt;&gt;&gt; parsed_timestamp = datetime.fromisoformat(release_timestamp.strip(&#34;Z&#34;)) | &gt;&gt;&gt; print(parsed_timestamp) | 2020-08-08 15:00:09 | | options() | :staticmethod: | Provides the options by calling the ``OPTIONS`` method of the API. | | Returns | - | dict | API options. | | Examples | -- | &gt;&gt;&gt; from pprint import pprint | &gt;&gt;&gt; options = Cov19API.options() | &gt;&gt;&gt; pprint(options) | {&#39;info&#39;: {&#39;description&#39;: &#34;Public Health England&#39;s Coronavirus Dashboard API&#34;, | &#39;title&#39;: &#39;Dashboard API&#39;, | &#39;version&#39;: &#39;1.0&#39;}, | &#39;openapi&#39;: &#39;3.0.1&#39;, | ... | } | | - | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) | | api_params | :staticmethod: | API parameters, constructed based on ``filters``, ``structure``, | and ``latest_by`` arguments as defined by the user. | | Returns | - | Dict[str, str] | | last_update | :property: | Produces the timestamp for the last update in GMT. | | This property supplies the API time - i.e. the time at which the data were | deployed to the database. Please note that there will always be a difference | between this time and the timestamp that is displayed on the website, which may | be accessed via the ``.get_release_timestamp()`` method. The website timestamp | signifies the time at which the data were release to the API, and by extension | the website. | | .. note:: | | The output is extracted from the header and is accurate to | the second. | | .. warning:: | | The ISO-8601 standard requires a ``&#34;Z&#34;`` character to be added | to the end of the timestamp. This is a timezone feature and is | not recognised by Python&#39;s ``datetime`` library. It is, however, | most other libraries; e.g. ``pandas``. If you wish to parse the | timestamp using the the ``datetime`` library, make sure that you | remove the trailing ``&#34;Z&#34;`` character. | | Returns | - | str | Timestamp, formatted as ISO-8601. | | Examples | -- | &gt;&gt;&gt; filters = [&#34;areaType=region&#34;] | &gt;&gt;&gt; structure = { | ... &#34;name&#34;: &#34;areaName&#34;, | ... &#34;newCases&#34;: &#34;newCasesBySpecimenDate&#34; | ... } | &gt;&gt;&gt; data = Cov19API( | ... filters=filters, | ... structure=structure, | ... latest_by=&#39;newCasesBySpecimenDate&#39; | ... ) | &gt;&gt;&gt; timestamp = data.last_update | &gt;&gt;&gt; print(timestamp) | 2020-07-27T20:29:16.000000Z | | &gt;&gt;&gt; from datetime import datetime | &gt;&gt;&gt; parsed_timestamp = datetime.fromisoformat(timestamp.strip(&#34;Z&#34;)) | &gt;&gt;&gt; print(parsed_timestamp) | 2020-07-27 20:29:16 | | total_pages | :property: | Produces the total number of pages for a given set of | parameters (only after the data are requested). | | Returns | - | Union[int, None] | | - | Data and other attributes defined here: | | __annotations__ = {&#39;_last_update&#39;: typing.Union[str, NoneType], &#39;_tota... | | endpoint = &#39;https://api.coronavirus.data.gov.uk/v1/data&#39; | | release_timestamp_endpoint = &#39;https://api.coronavirus.data.gov.uk/v1/t... . . So now we need to define two things: the filters and the structure. . Data Filters . The filter tells the API what kind of area we would like data about. Valid values for the filters are: . List of valid filtersareaTypeArea type as stringareaNameArea name as stringareaCodeArea Code as stringdateDate as string [YYYY-MM-DD] We must specify the areaType, so we will set it to nation. This will give us the data on the country level - so the total data for Wales, Scotland, Northen Ireland and England. . filter_all_nations = [ &quot;areaType=nation&quot; ] . filter_all_uk = [ &quot;areaType=overview&quot; ] . Other options for areaType will give: . overview overview data for the UK | region Region data (regions for England only) | nhsregion NHS region data (only England) | utla Upper-tier local authority data (Again, only England) | ltla Lower-tier local authority data (...only England) | . Data Structure . The structure parameter describes what metrics we want the data to describe. There are a lot of them, but the main metrics are areaName, date and newCasesByPublishDate. Click the arrow below to expand the full list of valid metrics. . See a list of valid metrics for structureareaTypeArea type as stringareaNameArea name as stringareaCodeArea Code as stringdateDate as string [YYYY-MM-DD]hashUnique ID as stringnewCasesByPublishDateNew cases by publish datecumCasesByPublishDateCumulative cases by publish datecumCasesBySpecimenDateRateRate of cumulative cases by publish date per 100k resident populationnewCasesBySpecimenDateNew cases by specimen datecumCasesBySpecimenDateRateRate of cumulative cases by specimen date per 100k resident populationcumCasesBySpecimenDateCumulative cases by specimen datemaleCasesMale cases (by age)femaleCasesFemale cases (by age)newPillarOneTestsByPublishDateNew pillar one tests by publish datecumPillarOneTestsByPublishDateCumulative pillar one tests by publish datenewPillarTwoTestsByPublishDateNew pillar two tests by publish datecumPillarTwoTestsByPublishDateCumulative pillar two tests by publish datenewPillarThreeTestsByPublishDateNew pillar three tests by publish datecumPillarThreeTestsByPublishDateCumulative pillar three tests by publish datenewPillarFourTestsByPublishDateNew pillar four tests by publish datecumPillarFourTestsByPublishDateCumulative pillar four tests by publish datenewAdmissionsNew admissionscumAdmissionsCumulative number of admissionscumAdmissionsByAgeCumulative admissions by agecumTestsByPublishDateCumulative tests by publish datenewTestsByPublishDateNew tests by publish datecovidOccupiedMVBedsCOVID-19 occupied beds with mechanical ventilatorshospitalCasesHospital casesplannedCapacityByPublishDatePlanned capacity by publish datenewDeaths28DaysByPublishDateDeaths within 28 days of positive testcumDeaths28DaysByPublishDateCumulative deaths within 28 days of positive testcumDeaths28DaysByPublishDateRateRate of cumulative deaths within 28 days of positive test per 100k resident populationnewDeaths28DaysByDeathDateDeaths within 28 days of positive test by death datecumDeaths28DaysByDeathDateCumulative deaths within 28 days of positive test by death datecumDeaths28DaysByDeathDateRateRate of cumulative deaths within 28 days of positive test by death date per 100k resident population We will look at new cases by publish date and new deaths by death date, so the structure will look like this . structure_cases_death = { &quot;date&quot;: &quot;date&quot;, &quot;areaName&quot;: &quot;areaName&quot;, &quot;newCases&quot;: &quot;newCasesByPublishDate&quot;, &quot;cumCases&quot;: &quot;cumCasesBySpecimenDate&quot;, &quot;cumCasesRate&quot;: &quot;cumCasesBySpecimenDateRate&quot;, &quot;newDeaths&quot;: &quot;newDeathsByDeathDate&quot; } . Pulling and cleaning data . Now we create the class and get the DataFrame from it. We also use fillna(0) to fill any entries that are NaN&#39;s - because that is the default if a value is missing. . uk_cases = Cov19API(filters=filter_all_nations, structure=structure_cases_death).get_dataframe().fillna(0) uk_cases[&#39;date&#39;] = pd.to_datetime(uk_cases[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) uk_cases.sort_values([&#39;areaName&#39;, &#39;date&#39;], inplace=True) uk_cases.reset_index(drop=True, inplace=True) . Note that the Welsh Government announced that 11,000 cases were missing from between the 9th and 15th of December. This explains the large spike after the 17th of December, and also the decrease in cases before that. See this BBC article and relevant announcement by Public Health Wales about how they are changing the way they report cases. . In the data from the COVID19 API, all 11,000 cases are allocated to the 17th of December. To overcome this, we will evenly distribute the cases out over the preceeding 5 days. This may not be the most accurate way of doing it, but it will result in the cleanest picture when it comes to plotting the graphs. . date_list = [&#39;2020-12-13&#39;, &#39;2020-12-14&#39;, &#39;2020-12-15&#39;, &#39;2020-12-16&#39;, &#39;2020-12-17&#39;] uk_cases.iloc[(uk_cases.query(&quot;areaName==&#39;Wales&#39;&quot;).query(&quot;date==@date_list&quot;).index), 2] = np.flip( np.array(list(range(2494 + int((2801 - 2494)/6), 2801 - int((2801 - 2494)/6), int((2801 - 2494)/6))))) . Finally we add a column to the dataframe called dailyChange which will keep track of if the number of new cases has gone up or down per day. . grouped_df = uk_cases.groupby(&#39;areaName&#39;) uk_cases[&#39;casesChange&#39;] = grouped_df.apply( lambda x: x[&#39;newCases&#39;] - x[&#39;newCases&#39;].shift(1).fillna(0)).reset_index(drop=True) uk_cases.sample(5, random_state=40) # a random sample of rows . date areaName newCases cumCases cumCasesRate newDeaths casesChange . 1456 2021-06-29 | Northern Ireland | 278.0 | 123959.0 | 6539.6 | 0 | 67.0 | . 92 2020-05-01 | England | 2501.0 | 155443.0 | 274.9 | 0 | 17.0 | . 1409 2021-05-13 | Northern Ireland | 99.0 | 118238.0 | 6237.8 | 0 | -14.0 | . 608 2021-09-29 | England | 29036.0 | 6830611.0 | 12078.9 | 0 | 228.0 | . 3503 2022-04-20 | Wales | 816.0 | 871480.0 | 27495.1 | 0 | 544.0 | . Notice that only deaths in England have been counted in the newDeaths column. I prefer to look at the number of cases per 100k population, but to do this with the newCases column, we would need to grab population data for each country. Alternatively we can estimate the population by using the cumulative cases per 100k column - the cases per 100k is given by . $$ text{cases per 100k} = 100000 * frac{ text{cases}}{ text{population}} $$ We will take numbers from the latest available day (just to make sure there are no zeros). For Wales: . wales_pop = round(100000 * uk_cases.query(&quot;areaName == &#39;Wales&#39;&quot;).cumCases.max() / uk_cases.query(&quot;areaName == &#39;Wales&#39;&quot;).cumCasesRate.max()) print(f&#39;Wales population: {wales_pop}&#39;) . Wales population: 3169583 . which is about right (it was 3,152,879 in 2019..). And for the rest of the countries: . countries = [&#39;Wales&#39;, &#39;Scotland&#39;, &#39;Northern Ireland&#39;, &#39;England&#39;] countries_population = dict() for country in countries: countries_population[country] = round(100000 * uk_cases.query( &quot;areaName == @country&quot;).cumCases.max() / uk_cases.query(&quot;areaName == @country&quot;).cumCasesRate.max()) if &#39;population&#39; not in uk_cases.columns: countries_pop_df = pd.DataFrame.from_dict(countries_population, orient=&#39;index&#39;, columns=[ &#39;population&#39;]) uk_cases = uk_cases.join(countries_pop_df, on=&#39;areaName&#39;) uk_cases[&#39;newCasesRate&#39;] = 100000 * uk_cases.newCases / uk_cases.population uk_cases[&#39;casesChangeRate&#39;] = 100000 * uk_cases.casesChange / uk_cases.population uk_cases = uk_cases.query(&quot;date &gt;= &#39;2020-09-01&#39;&quot;) . After all that, we just added some new columns that use the &#39;per 100k&#39; metric. The last thing we will add is a column showing the number of cases over a 7 day period. . Weekly cases . We will take the 7 day rolling sum of the new cases rate (i.e, new cases per 100k population) grouped by each country, and fill the missing values with 0&#39;s. . uk_cases[&#39;weeklyCasesRate&#39;] = uk_cases.groupby(by=&#39;areaName&#39;)[&#39;newCasesRate&#39;].rolling(7).sum().reset_index(drop=True).fillna(0) . Overview of UK cases . For the plotting we will also take the total cases for the UK. We could do this by grouping by date in the uk_cases dataframe and summing up the new cases like that - however we will just run another query with the Cov19API and run the same preprocessing as above, but this time cleaned up into a function. . overview_cases = Cov19API(filters=filter_all_uk, structure=structure_cases_death).get_dataframe().fillna(0) def preprocess_dataframe(df): df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&#39;%Y-%m-%d&#39;) df.sort_values(&#39;date&#39;, inplace=True) df.reset_index(drop=True, inplace=True) df[&#39;casesChange&#39;] = df[&#39;newCases&#39;] - df[&#39;newCases&#39;].shift(-1).fillna(0) population = round(100000 * df.cumCases.max() / df.cumCasesRate.max()) df[&#39;newCasesRate&#39;] = 100000 * df.newCases / population df[&#39;casesChangeRate&#39;] = 100000 * df.casesChange / population df[&#39;weeklyCasesRate&#39;] = df[&#39;newCasesRate&#39;].rolling(7).sum().fillna(0) return df preprocess_dataframe(overview_cases) overview_cases = overview_cases.query(&quot;date &gt;= &#39;2020-09-01&#39;&quot;) . Plotting the data . We will use the Python library Altair for visualising the data, see the altair docs for more information. . First we have a graph which shows the daily change in the number of new cases for each country. This number jumps up and down all over the place, which is likely due to delay in reporting of new cases over the weekend. Another interesting thing is that it looks like the daily cases in Wales experienced a much shorter period of calm over the summer (calm in the sense of daily cases not jumping up and down). . The orange bars are days when the number of new cases (per 100k population) was more than the previous day, while the blue are days when the number of new cases dropped. The red line is the 7 day moving average. . When the moving average line is below 0, it means that there is a consistent drop in new cases. We can see this clearly happening around the times that lockdowns were introduced (though, to varying degrees). I will update the graphs soon with a marker of when each lockdown started. . import altair as alt bars = alt.Chart(uk_cases).mark_bar().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;casesChangeRate:Q&quot;, axis=alt.Axis(title=&#39;Change in daily cases per 100k&#39;)), tooltip=&#39;casesChange&#39;, color=alt.condition( alt.datum.casesChangeRate &gt; 0, alt.value(&quot;orange&quot;), # The positive color alt.value(&quot;blue&quot;) # The negative color ) ).properties(title=&#39;Daily change in number of new cases with 7 day rolling mean&#39;,width=700).interactive() line = alt.Chart(uk_cases).mark_line( color=&#39;red&#39;, size=2, opacity=0.6 ).transform_window( rolling_mean=&#39;mean(casesChangeRate)&#39;, frame=[0, 7], groupby=[&#39;areaName&#39;] ).encode( x=&#39;yearmonthdate(date):T&#39;, y=&#39;rolling_mean:Q&#39; ) alt.layer(bars, line, data=uk_cases).facet(alt.Column( &#39;areaName&#39;, title=&#39;&#39;), columns=1).resolve_scale(y=&#39;independent&#39;).properties(title=&#39;Daily change in number of new cases in each country with 7 day rolling mean line&#39;) . . Next is a bar chart of the number of new cases in each country (per 100k population), with the 7 day moving average of cases. Again we see that Wales saw a longer period of raising and falling cases compared to the other countries. . After a period of cases falling, each nation is now seeing a rise in the number of cases - especially in Wales. . bars = alt.Chart(uk_cases).mark_bar().encode( x=alt.X(&quot;yearmonthdate(date):T&quot;, axis=alt.Axis(title=&#39;Date&#39;)), y=alt.Y(&quot;newCasesRate:Q&quot;, axis=alt.Axis(title=&#39;Daily new cases per 100k&#39;)), tooltip=&#39;newCasesRate&#39;, color=alt.condition( alt.datum.dailyChange &gt; 0, alt.value(&quot;orange&quot;), # The positive color alt.value(&quot;blue&quot;) # The negative color ) ).properties(title=&#39;New cases per 100k population with rolling 7 day average&#39;, width=600).interactive() line = alt.Chart(uk_cases).mark_line( color=&#39;red&#39;, size=2, ).transform_window( rolling_mean=&#39;mean(newCasesRate)&#39;, frame=[0, 7], groupby=[&#39;areaName&#39;] ).encode( x=&#39;yearmonthdate(date):T&#39;, y=&#39;rolling_mean:Q&#39; ) alt.layer(line, bars, data=uk_cases).facet(alt.Row(&#39;areaName&#39;, title=&#39;Country&#39;), columns=1) . .",
            "url": "https://idjotherwise.github.io/nlp-otherwise/covid/eda/2020/11/29/covid-charting.html",
            "relUrl": "/covid/eda/2020/11/29/covid-charting.html",
            "date": " • Nov 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a freelance data scientist, specialising in Natural Language Processing and Machine Learning. My background is in pure mathematics, having obtained a PhD in 2019 - my mathematics articles from my time as a PhD student can be found on arxiv. . My main interest is in NLP, where I am exploring multi-lingual sentiment analysis models. When I’m playing around with data, I enjoy building small websites (with fastAPI), skiing (in the Austrian Alps) and playing games (both computer and board!) . If you would like to get in touch, you can reach me at my email on the bottom of this page. .",
          "url": "https://idjotherwise.github.io/nlp-otherwise/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://idjotherwise.github.io/nlp-otherwise/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}